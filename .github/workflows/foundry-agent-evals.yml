name: Azure AI Foundry Agent Evaluations

on:
  workflow_dispatch:
    
permissions:
  id-token: write
  contents: read
  models: read

jobs:
  run-action:
    name: Run Azure AI Foundry Agent Evaluations
    runs-on: ubuntu-latest
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    # - name: Log in to Azure
    #   uses: azure/login@v2
    #   with:
    #     client-id: ${{ secrets.AZURE_CLIENT_ID }}
    #     tenant-id: ${{ secrets.AZURE_TENANT_ID }}
    #     subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
    
    - name: store eval data
      id: store-eval-data
      run: |
        DATA_FILE=$(cat data/agent-evals.json)
        echo DATA_FILE=$DATA_FILE >> $GITHUB_OUTPUT

    - name: GitHub Models Eval Summary
      id: inference
      uses: actions/ai-inference@v1
      with:
        system-prompt: |
          You are an AI assistant that is knowledgeable about the evaluation of AI agents in Azure AI Foundry. 
          Your task is to analyze the evaluation criteria and summarize the evaluation process.
          You should explain the importance of this evaluation for the development and deployment of AI agents to someone who is not familiar with the topic.
        prompt: |
          Analyze the criteria for evaluating the performance of AI agents in Azure AI Foundry. 
          Provide a summary of the evaluation process and its importance. 
          Use the eval data in ${{ steps.store-eval-data.outputs.DATA_FILE }}. 
          Explain why this evaluation is important for the development and deployment of AI agents.
          Keep the summary concise and focused on the key aspects of the evaluation.
          Keep the response under 200 words, but use bullets to highlight key points.
    
    - name: Add Model Eval Summary to Job Summary
      id: output
      run: |
        cat "${{ steps.inference.outputs.response-file }}" >> $GITHUB_STEP_SUMMARY


    # - name: Azure AI Agent Evaluation
    #   uses: microsoft/ai-agent-evals@v1-beta
    #   with:
    #     deployment-name: "gpt-4o-mini"
    #     azure-aiproject-connection-string: ${{ secrets.AZURE_AI_PROJECT_CONNECTION_STRING }}
    #     agent-ids: ${{ secrets.AZURE_AI_AGENT_IDS }}
    #     # evaluation-result-view: "all-scores"
    #     data-path: "data/agent-evals.json"